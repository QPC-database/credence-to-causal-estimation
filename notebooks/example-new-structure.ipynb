{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREDENCE -  Credence to Causal Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premise. \n",
    "Retrospective causal inference methods, like synthetic controls or difference in difference, are widely used to estimate the impact of an intervention where A/B testing is not an option: for example, fee changes or marketing campaigns. The causal effect of an intervention is the difference between the outcome under a treatment and the counterfactual outcome if an alternative treatment (or no treatment) was chosen. The main challenge in retrospective and non-experimental causal inference is that we only observe the outcome under one of the treatment choices while the counterfactual is never observed. Causal inference methods aims to re-create the scenario in which the alternative treatment was chosen i.e. estimating the counterfactual.\n",
    "\n",
    "### Evaluation. \n",
    "Traditional causal inference methods are typically evaluated in two ways: placebo tests and tests on simulated data. Placebo tests check that the treatment effect estimated by a particular method is zero during a period there is actually no intervention: in that situation, the actual should match the counterfactual. Testing on simulated data is limited because the data generative process is often simple and lacks the complexities of real world datasets where the method is actually applied. For applied researchers, the important question is understanding how well a method actually performs in realistic situations, not on a toy dataset.\n",
    "\n",
    "### Framework. \n",
    "In this project, we propose and develop a framework for generating complex and realistic datasets with known treatment effects. This approach combines the best parts of placebo tests and simulation tests: the simulated dataset would have as much complexity as the real world datasets used in placebo tests, but users would be able to control the treatment effect as well as the level of endogeneity like they do in simulation tests.\n",
    "\n",
    "Thus, generated datasets can then be used to understand the performance of different causal inference methods on various metrics, allowing scientists to choose appropriate method for a given problem. Currently, we focus on causal inference with time-series data using synthetic control method(s). \n",
    "\n",
    "Our framework uses a neural network based black-box data generative model called Autoregressive Variational AutoEncoder (AR-VAE), and an interpretable transformation map (ITM) to learn the distribution and sample dataset which have similar dynamics as the real datasets of interest. The AR-VAE model allows us to generate complex data while the ITM allows us to manipulate and intervene on the samples to encode treatment effects. \n",
    "\n",
    "Code Commit Link - https://git-codecommit.us-east-1.amazonaws.com/v1/repos/credence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisite python libraries\n",
    "Numpy, Math, Pytorch, Pytorchvision, Pytorch-lightning, Pandas, Matplotlib, Sys, OS, sbibm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add path for repo\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(os.path.abspath('../python'))\n",
    "import train_arvae\n",
    "import verify_causal_model\n",
    "\n",
    "from train_arvae import exclude_small_targets, prepare_input, generate_example_sample\n",
    "from train_arvae import rescale, convert_to_2d\n",
    "from sbibm.metrics.mmd import mmd\n",
    "from validate_arvae import c2st_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use CREDENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Training Data\n",
    "Read training data and store it in form (T,B,N) where T is the number of time steps, B is the number of units, N is the number of time-series in a unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "input_file = '../data/traindata_US_referral_V1_rm_zero.pkl'\n",
    "# This file can be downloaded from s3://sdg-credence/data/referral_V1/processed_pkl/traindata_US_referral_V1_rm_zero.pkl\n",
    "with open(input_file, 'rb') as file:\n",
    "    data_input = dill.load(file)\n",
    "data_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=data_input.shape[1]\n",
    "N=52\n",
    "T = data_input.shape[0] #length timeseries\n",
    "targets = 2\n",
    "adjust = 10\n",
    "\n",
    "import dill\n",
    "in_file = '../data/traindata_US_referral_V1_postprocessed_rm_zero_run2.pkl'\n",
    "# This file can be downloaded from s3://sdg-credence/data/referral_V1/processed_pkl/traindata_US_referral_V1_postprocessed_rm_zero_run2.pkl\n",
    "with open(in_file, 'rb') as file:\n",
    "    data = dill.load(file)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = exclude_small_targets(data = data, targets=targets)\n",
    "data_norm_orig = prepare_input(data = data, targets=targets, adjust=adjust, outlier_threshold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_norm, data_norm_test = train_test_split(data_norm_orig, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ARVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {}\n",
    "hyper_params['epochs'] = 400\n",
    "hyper_params['kld_weight'] = 5e-06\n",
    "hyper_params['hidden_dims'] = [512,256,128]\n",
    "hyper_params['latent_dim'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Running training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_model, runner = train_arvae.train(data_norm, hyper_params, \n",
    "                          output_checkpoint_path='ar_vae.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load from check point\n",
    "# import t_VAE\n",
    "# vae_model = t_VAE.AR_VAE.load_from_checkpoint(checkpoint_path = 'ar_vae.ckpt',\n",
    "#                                                       X = torch.tensor(data_norm).float(),\n",
    "#                                                       latent_dim = hyper_params['latent_dim'] , \n",
    "#                                                       hidden_dims = hyper_params['hidden_dims'],\n",
    "#                                                       kl_weights = hyper_params['kld_weight'],\n",
    "#                                                          ).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to generate samples from latent space\n",
    "targets_var = generate_example_sample(vae_model, targets=targets, adjust=adjust,T=T,B=B,N=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arvae.plot_data(vae_model=vae_model, targets=targets, adjust=adjust, T=T, N=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arvae.plot_latent_space(vae_model, data_norm_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate reference samples and algorithm samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_test = data_norm_test.shape[0]\n",
    "reference_samples = rescale(data_norm_test, targets=targets ,B=B_test, N=N, T=T, adjust=adjust)\n",
    "reference_samples = convert_to_2d(reference_samples)\n",
    "reference_samples = torch.tensor(reference_samples).float()\n",
    "algorithm_samples = generate_example_sample(vae_model, targets=targets, B=B_test, N=N, T=T, adjust=adjust)\n",
    "algorithm_samples = convert_to_2d(algorithm_samples)\n",
    "algorithm_samples=torch.tensor(algorithm_samples).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Maximum Mean Discrepancy (MMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmd_accuracy = mmd(reference_samples, algorithm_samples)\n",
    "print(\"MMD2: \", mmd_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C2ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## c2st\n",
    "#     reference_samples = torch.tensor(data_norm_orig).float()\n",
    "c2st_accuracy = c2st_new(reference_samples, algorithm_samples, z_score=False, hidden_layer_sizes=(64,32))\n",
    "print(\"C2ST: \", c2st_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
